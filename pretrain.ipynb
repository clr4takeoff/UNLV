{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df559d6ecf587ef4",
   "metadata": {},
   "source": [
    "# Pre-Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6487069a22b443e",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, mkdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebb5e60434fc75",
   "metadata": {},
   "source": [
    "### Huggingface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96406df8351e29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment out the line below when you need to login to Huggingface\n",
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88444867b0b14178",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e70cad906b1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5ca44145e20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "device = torch.device(f\"cuda:{DEVICE_NUM}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f33be18440cabc",
   "metadata": {},
   "source": [
    "## Load DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41127ff906823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import ImageNet1K, CIFAR100, CIFAR10, DatasetHolder, build_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d472ec0525e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "IMAGENETs = DatasetHolder(\n",
    "    train=ImageNet1K(root=DATA_ROOT, force_download=False, train=True, transform=build_augmentation(ImageNet1K.img_size)),\n",
    "    valid=ImageNet1K(root=DATA_ROOT, force_download=False, valid=True),\n",
    "    test=ImageNet1K(root=DATA_ROOT, force_download=False, train=False)\n",
    ").split_train_attack()\n",
    "print(f\"INFO: Dataset loaded successfully - {IMAGENETs}\")\n",
    "\n",
    "CIFAR100s = DatasetHolder(\n",
    "    train=CIFAR100(root=DATA_ROOT, download=True, train=True, transform=build_augmentation(CIFAR100.img_size)),\n",
    "    test=CIFAR100(root=DATA_ROOT, download=True, train=False)\n",
    ").split_train_valid().split_train_attack()\n",
    "print(f\"INFO: Dataset loaded successfully - {CIFAR100s}\")\n",
    "\n",
    "CIFAR10s = DatasetHolder(\n",
    "    train=CIFAR10(root=DATA_ROOT, download=True, train=True, transform=build_augmentation(CIFAR10.img_size)),\n",
    "    test=CIFAR10(root=DATA_ROOT, download=True, train=False)\n",
    ").split_train_valid().split_train_attack()\n",
    "print(f\"INFO: Dataset loaded successfully - {CIFAR10s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfaaf862a9a0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_DATASET =  CIFAR10s\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = CHOSEN_DATASET.train, CHOSEN_DATASET.valid, CHOSEN_DATASET.test\n",
    "print(f\"INFO: Dataset Size - {CHOSEN_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f6925761e5e63",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd53b09a07e7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "#BATCH_SIZE = 512, 512, 512\n",
    "BATCH_SIZE = len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5748cf1442804",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178d4118a670b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.show_sample_grid(**CHOSEN_DATASET.config.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5ea2a2af0c35d",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddcb2d583a637f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import (\n",
    "    BaseModel,\n",
    "    # Target Models\n",
    "    ViTBase, ViTLarge, SwinTiny, SwinSmall,\n",
    "    # Compare Models\n",
    "    ConvNeXtTiny, ConvNeXtSmall, ResNet50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetModel: BaseModel = SwinSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3500011660e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model (automatically loads ImageNet pretrained weights)\n",
    "TargetModel.dataset_name = CHOSEN_DATASET.train.name\n",
    "model = TargetModel.from_pretrained()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f914f6148a87ae7",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ecd5b0e916b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(lst):\n",
    "    try:\n",
    "        return sum(lst) / len(lst)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd00e380308053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = CHOSEN_DATASET.config.epoch\n",
    "LEARNING_RATE = 1e-4, 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fd351c87211b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "try:\n",
    "    epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "    with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "\n",
    "        for epoch in epochs:\n",
    "            train_progress.reset(total=train_length)\n",
    "            valid_progress.reset(total=valid_length)\n",
    "\n",
    "            train_acc, train_loss, val_acc, val_loss = [], [], [], []\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "            for i, (inputs, targets) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "                train_acc.append((torch.max(outputs, 1)[1] == targets.data).sum().item() / len(inputs))\n",
    "\n",
    "                train_progress.update(1)\n",
    "                print(f\"\\rEpoch [{epoch+1:4}/{EPOCHS:4}], Step [{i+1:4}/{train_length:4}], Acc: {avg(train_acc):.6%}, Loss: {avg(train_loss):.6f}\", end=\"\")\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in valid_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)  # but not use model loss\n",
    "\n",
    "                    val_loss.append(criterion(outputs, targets).item())\n",
    "                    val_acc.append((torch.max(outputs, 1)[1] == targets.data).sum().item() / len(inputs))\n",
    "                    valid_progress.update(1)\n",
    "\n",
    "            print(f\"\\rEpoch [{epoch+1:4}/{EPOCHS:4}], Step [{train_length:4}/{train_length:4}], Acc: {avg(train_acc):.6%}, Loss: {avg(train_loss):.6f}, Valid Acc: {avg(val_acc):.6%}, Valid Loss: {avg(val_loss):.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nINFO: Training interrupted by user. Saving model...\")\n",
    "finally:\n",
    "    # Model Save\n",
    "    save_directory = model.save_pretrained()  # Saves to \"./results/\"\n",
    "    print(f\"INFO: Model saved successfully at {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762d126105c4809",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a4c75ebeb969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = TargetModel.from_pretrained(save_directory, num_classes=train_dataset.num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf5249a208f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        corrects += (preds == targets.data).sum()\n",
    "        print(f\"Model Accuracy: {corrects/test_length:%}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256aa0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
